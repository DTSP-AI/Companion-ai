import { NextRequest, NextResponse } from "next/server";
import { getAuth } from "@clerk/nextjs/server";
import { ChatOpenAI } from "@langchain/openai";
import { ConversationChain } from "langchain/chains";
import {
  ChatPromptTemplate,
  HumanMessagePromptTemplate,
  SystemMessagePromptTemplate,
  MessagesPlaceholder,
} from "@langchain/core/prompts";
import { MemoryManager, CompanionKey } from "@/lib/memory";
import prismadb from "@/lib/prismadb";
import { rateLimit } from "@/lib/rate-limit";

console.log("üîß [DEBUG] API route loaded");
console.log("üîß [DEBUG] Process environment variables:");
console.log(
  "üîß [DEBUG] OPENAI_API_KEY is",
  process.env.OPENAI_API_KEY ? "set" : "NOT set"
);

const conversationChains = new Map<string, ConversationChain>();

const getCharacterDescription = (description: any): string => {
  console.log("üîß [DEBUG] Entered getCharacterDescription()");
  console.log("üîç [getCharacterDescription] Processing character description");
  const result = description
    ? JSON.stringify(description)
    : "No character description provided.";
  console.log(
    `üìù [getCharacterDescription] Result: ${result.substring(0, 50)}...`
  );
  console.log("üîß [DEBUG] Exiting getCharacterDescription()");
  return result;
};

const createConversationChain = (
  characterDescription: string,
  bufferMemory: any
) => {
  console.log("üîß [DEBUG] Entered createConversationChain()");

  if (!process.env.OPENAI_API_KEY) {
    console.error("‚ùå [ERROR] OPENAI_API_KEY is not set in environment variables");
    throw new Error("OPENAI_API_KEY is not set");
  } else {
    console.log("‚úÖ [INFO] OPENAI_API_KEY is set");
  }

  console.log("üîó [CREATE_CHAIN] Creating new conversation chain");
  const llm = new ChatOpenAI({
    modelName: "gpt-4",
    temperature: 0.9,
    openAIApiKey: process.env.OPENAI_API_KEY,
  });
  console.log(`ü§ñ [CREATE_CHAIN] LLM created with model: ${llm.modelName}`);

  const prompt = ChatPromptTemplate.fromMessages([
    SystemMessagePromptTemplate.fromTemplate(characterDescription),
    new MessagesPlaceholder("shortTermHistory"),
    HumanMessagePromptTemplate.fromTemplate("{input}"),
  ]);
  console.log("üìù [CREATE_CHAIN] Prompt template created");

  const chain = new ConversationChain({
    prompt: prompt,
    llm: llm,
    memory: bufferMemory,
    verbose: true,
  });
  console.log("‚úÖ [CREATE_CHAIN] Conversation chain created successfully");

  console.log("üîß [DEBUG] Exiting createConversationChain()");
  return chain;
};

export async function POST(
  req: NextRequest,
  { params }: { params: { chatId: string } }
) {
  console.log("üöÄ [POST] Starting POST request handling for chatId:", params.chatId);
  try {
    // Parse the request body
    const { prompt } = await req.json();
    console.log(`üì• [POST] Received prompt: "${prompt}"`);

    // Get user ID
    const { userId } = getAuth(req);
    console.log(`üë§ [POST] User ID: ${userId}`);

    if (!userId) {
      console.log("üö´ [POST] Unauthorized access attempt");
      return new NextResponse("Unauthorized", { status: 401 });
    }

    // Rate limiting
    const { success } = await rateLimit(req);
    console.log(`üö¶ [POST] Rate limit check result: ${success}`);
    if (!success) {
      console.log("üõë [POST] Rate limit exceeded");
      return new NextResponse("Rate limit exceeded", { status: 429 });
    }

    // Fetch companion data
    console.log(`üîç [POST] Fetching companion data for chatId: ${params.chatId}`);
    const companion = await prismadb.companion.findUnique({
      where: { id: params.chatId },
      include: { messages: true },
    });

    if (!companion) {
      console.log("‚ùå [POST] Companion not found");
      return new NextResponse("Companion not found", { status: 404 });
    }
    console.log(`‚úÖ [POST] Companion found: ${companion.id}`);

    // Process character description
    const characterDescription = getCharacterDescription(
      companion.characterDescription
    );
    console.log(`üìù [POST] Character description processed`);

    // Initialize MemoryManager
    console.log("üß† [POST] Initializing MemoryManager");
    const memoryManager = await MemoryManager.getInstance();

    const companionKey: CompanionKey = {
      companionId: companion.id,
      userId: userId,
    };
    console.log(`üîë [POST] CompanionKey created: ${JSON.stringify(companionKey)}`);

    // Create or reuse conversation chain
    let chain = conversationChains.get(params.chatId);
    if (!chain) {
      console.log("üÜï [POST] Creating new conversation chain");
      const { bufferMemory } = await memoryManager.getMemoryManager(companionKey);
      console.log("üîß [DEBUG] Obtained bufferMemory from MemoryManager");
      chain = createConversationChain(characterDescription, bufferMemory);
      conversationChains.set(params.chatId, chain);
      console.log("‚úÖ [POST] New conversation chain created and stored");
    } else {
      console.log("‚ôªÔ∏è [POST] Reusing existing conversation chain");
    }

    // Determine if long-term memory should be used
    const messageCount = companion.messages.length;
    console.log(`üìä [POST] Total message count: ${messageCount}`);
    let relevantMemories: string[] = [];
    if (messageCount >= 10) {
      console.log("üîç [POST] Retrieving relevant long-term memories");
      relevantMemories = await memoryManager.getRelevantMemories(
        companionKey,
        prompt
      );
      console.log(`üìö [POST] Retrieved ${relevantMemories.length} relevant memories`);
    } else {
      console.log(
        "üîç [POST] Skipping long-term memory retrieval as message count is less than 10"
      );
    }

    // Generate AI response
    console.log("üí¨ [POST] Generating AI response with prompt:", prompt);
    console.log("üîß [DEBUG] About to call chain.call()");
    const response = await chain.call({
      input: prompt,
      longTermHistory: relevantMemories.join("\n") || "",
    });
    console.log("üîß [DEBUG] Received response from chain.call()");
    const aiMessage = response.response.trim();
    console.log(`ü§ñ [POST] AI response generated: "${aiMessage}"`);

    // Store user message in database
    console.log("üíæ [POST] Storing user message in database");
    await prismadb.message.create({
      data: {
        content: prompt,
        role: "user",
        userId: userId,
        companionId: companion.id,
      },
    });

    // Store AI response in database
    console.log("üíæ [POST] Storing AI response in database");
    await prismadb.message.create({
      data: {
        content: aiMessage,
        role: "system",
        userId: userId,
        companionId: companion.id,
      },
    });

    // Update the agent memory with the latest conversation
    if (messageCount >= 10) {
      console.log("üß† [POST] Updating agent memory with latest conversation");
      await memoryManager.updateAgentMemory(companionKey, prompt, aiMessage);
      console.log("‚úÖ [POST] Agent memory updated");
    } else {
      console.log(
        "üß† [POST] Skipping agent memory update as message count is less than 10"
      );
    }

    console.log("üèÅ [POST] Request handling completed successfully");
    return NextResponse.json({ systemMessage: aiMessage });
  } catch (error) {
    console.error("‚ùå [POST_ERROR] An error occurred during request processing:");
    if (error instanceof Error) {
      console.error(`Error name: ${error.name}`);
      console.error(`Error message: ${error.message}`);
      console.error(`Error stack: ${error.stack}`);
    } else {
      console.error("Error details:", JSON.stringify(error, null, 2));
    }

    return NextResponse.json(
      {
        error: "An unexpected error occurred",
        details: error instanceof Error ? error.message : "Unknown error",
      },
      {
        status: 500,
      }
    );
  }
}
